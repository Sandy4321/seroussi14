'''
Created on Sep 14, 2015

@author: thomas
'''
import string
import nltk
import h5py
import numpy as np

class Corpus(object):
    '''
    Class to store a text corpus.
    '''
    
    def __init__(self,name):
        '''
        Constructor
        '''
        self.documents = []
        self.name = name
        
        # Initialize corpus variables.
        self.V = 0      # Number of words in vocabulary.
        self.D = 0      # Number of documents.
        self.A = 0      # Number of authors.
        self.authors = {}
        self.is_stopword = []   # List of stopword identifier. 0 if not, 1 if its a stopword.
        self.num_stopwords = 0  # Counter for the number of stopwords in the vocabulary.
            
        
    def index_douments(self):
        '''
        Apply an integer index to documents for use in model.
        '''
        # Loop over all document and assign the index according to the corpus' document list.
        for index, doc in enumerate(self.documents):
            doc.index = index
        
        
    def word_tokenize(self):
        '''
        For every document built a list of tokens out of the documents' text.
        '''
        for doc in self.documents:
            doc.word_tokenize()
            
            
    def built_vocabluary(self):
        '''
        Built a list of tokens in the corpus.
        The word_tokenize functions must be called prior to this function.
        '''
        # Create an empty set.
        vocab_set = set()
        
        # Loop over documents.
        for doc in self.documents:
            # Loop over tokens and add each word to the set.
            for word in doc.tokens:
                vocab_set.add(word)
        # Update vocabulary size variable.
        self.V = len(vocab_set)
        # Store the vocabulary in a dict.
        self.vocab_dict = dict(zip(vocab_set,range(self.V)))
        # Store the vocabulary in a list with indices according to their values in the dict.
        self.vocab = [None]*self.V
        for word in self.vocab_dict.keys():
            self.vocab[self.vocab_dict[word]] = word
        
        
    def index_words(self):
        '''
        Apply the integer index for every word according to the vocabulary dict.
        '''
        
        # Loop over all documents.
        for doc in self.documents:
            # Loop over all words in a document.
            for word in doc.tokens:
                doc.words.append(self.vocab_dict[word])
      
      
    def find_all_authors(self):
        '''
        Construct a dict containing author IDs as keys and integer indices.
        '''
        # Initialize a set.
        authorID_set = set()
        # Loop over all documents and add the author to the set.
        for doc in self.documents:
            authorID_set.add(doc.authorID)
        # Update the number of authors.
        self.A = len(authorID_set)
        # Construct the dict containing IDs as keys and integer indices.
        self.authors = dict(zip(authorID_set,range(self.A)))
      
    def index_authors(self):
        '''
        Store the index of every documents author in the document object.
        '''
        # Loop over all documents.
        for doc in self.documents:
            doc.author_index = self.authors[doc.authorID]
            
    
    def identify_stopwords(self):
        '''
        Identify whether a word is a stop word according to a given list of stopwords.
        '''
        # Stop Word List 2
        # http://www.lextek.com/manuals/onix/stopwords2.html
        # This stopword list was built by Gerard Salton and Chris Buckley for the experimental SMART information retrieval system at Cornell University. This stopword list is generally considered to be on the larger side and so when it is used, some implementations edit it so that it is better suited for a given domain and audience while others use this stopword list as it stands. This wordlist is 571 words in length.
        # Freely available stopword list generated by 
        # Chris Buckley and Gerard Salton at Cornell University.
        stop_words = {"a","a's","able","about","above","according","accordingly","across","actually","after","afterwards","again","against","ain't","all","allow","allows","almost","alone","along","already","also","although","always","am","among","amongst","an","and","another","any","anybody","anyhow","anyone","anything","anyway","anyways","anywhere","apart","appear","appreciate","appropriate","are","aren't","around","as","aside","ask","asking","associated","at","available","away","awfully","b","be","became","because","become","becomes","becoming","been","before","beforehand","behind","being","believe","below","beside","besides","best","better","between","beyond","both","brief","but","by","c","c'mon","c's","came","can","can't","cannot","cant","cause","causes","certain","certainly","changes","clearly","co","com","come","comes","concerning","consequently","consider","considering","contain","containing","contains","corresponding","could","couldn't","course","currently","d","definitely","described","despite","did","didn't","different","do","does","doesn't","doing","don't","done","down","downwards","during","e","each","edu","eg","eight","either","else","elsewhere","enough","entirely","especially","et","etc","even","ever","every","everybody","everyone","everything","everywhere","ex","exactly","example","except","f","far","few","fifth","first","five","followed","following","follows","for","former","formerly","forth","four","from","further","furthermore","g","get","gets","getting","given","gives","go","goes","going","gone","got","gotten","greetings","h","had","hadn't","happens","hardly","has","hasn't","have","haven't","having","he","he's","hello","help","hence","her","here","here's","hereafter","hereby","herein","hereupon","hers","herself","hi","him","himself","his","hither","hopefully","how","howbeit","however","i","i'd","i'll","i'm","i've","ie","if","ignored","immediate","in","inasmuch","inc","indeed","indicate","indicated","indicates","inner","insofar","instead","into","inward","is","isn't","it","it'd","it'll","it's","its","itself","j","just","k","keep","keeps","kept","know","knows","known","l","last","lately","later","latter","latterly","least","less","lest","let","let's","like","liked","likely","little","look","looking","looks","ltd","m","mainly","many","may","maybe","me","mean","meanwhile","merely","might","more","moreover","most","mostly","much","must","my","myself","n","name","namely","nd","near","nearly","necessary","need","needs","neither","never","nevertheless","new","next","nine","no","nobody","non","none","noone","nor","normally","not","nothing","novel","now","nowhere","o","obviously","of","off","often","oh","ok","okay","old","on","once","one","ones","only","onto","or","other","others","otherwise","ought","our","ours","ourselves","out","outside","over","overall","own","p","particular","particularly","per","perhaps","placed","please","plus","possible","presumably","probably","provides","q","que","quite","qv","r","rather","rd","re","really","reasonably","regarding","regardless","regards","relatively","respectively","right","s","said","same","saw","say","saying","says","second","secondly","see","seeing","seem","seemed","seeming","seems","seen","self","selves","sensible","sent","serious","seriously","seven","several","shall","she","should","shouldn't","since","six","so","some","somebody","somehow","someone","something","sometime","sometimes","somewhat","somewhere","soon","sorry","specified","specify","specifying","still","sub","such","sup","sure","t","t's","take","taken","tell","tends","th","than","thank","thanks","thanx","that","that's","thats","the","their","theirs","them","themselves","then","thence","there","there's","thereafter","thereby","therefore","therein","theres","thereupon","these","they","they'd","they'll","they're","they've","think","third","this","thorough","thoroughly","those","though","three","through","throughout","thru","thus","to","together","too","took","toward","towards","tried","tries","truly","try","trying","twice","two","u","un","under","unfortunately","unless","unlikely","until","unto","up","upon","us","use","used","useful","uses","using","usually","uucp","v","value","various","very","via","viz","vs","w","want","wants","was","wasn't","way","we","we'd","we'll","we're","we've","welcome","well","went","were","weren't","what","what's","whatever","when","whence","whenever","where","where's","whereafter","whereas","whereby","wherein","whereupon","wherever","whether","which","while","whither","who","who's","whoever","whole","whom","whose","why","will","willing","wish","with","within","without","won't","wonder","would","would","wouldn't","x","y","yes","yet","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves","z","zero"}
        
        # Loop over all words.
        for word in self.vocab:
            if word in stop_words:
                self.is_stopword.append(1)
                # Update counter.
                self.num_stopwords += 1
            else:
                self.is_stopword.append(0)
      
      
    def process_raw_data(self):
        '''
        Apply all the steps necessary to obtain data in the form needed for the algorithm.
        '''
        # Word processing.
        self.word_tokenize()
        self.built_vocabluary()
        self.index_words()
        self.identify_stopwords()
        # Author processing.
        self.find_all_authors()
        self.index_authors()
        
        
    def print_status(self):
        '''
        Print some information about the corpus.
        '''
        print("Vocabulary size : {}".format(self.V))
        print("Number of documents : {}".format(self.D)) 
        print("Number of axuthors : {}".format(self.A))
        print("Number of stopwords : {} of 571 in stopword list.".format(self.num_stopwords))

         
    def save_to_hdf5(self):
        '''
        Save the complete corpus to an hdf5 file.
        '''
        filename = self.name + '.h5'
        # Create new file, let fail if exists.
        f = h5py.File(filename, 'x')
        
        corpus_group = f.create_group('corpus')
        corpus_group.attrs['name'] = self.name
        corpus_group.attrs['info'] = 'Created from PAN11 LargeTrain data set excluding LargeTrain/a8964.xml and LargeTrain/a7655.xml. Words tokenized using nltk.word_tokenize() and normalized using string.lower().'
        
        group_documents = corpus_group.create_group('documents')
        group_documents.attrs['info'] = 'Group of documents, each labeled by their integer index, containing a vector of words in form of integer labels.'
        
        authors_vec = [] # Vector containing author labels for each document.
        N = []      # Vector containing number of words for each document.


        for ind, doc in enumerate(self.documents):
            # Safe words vector to the database.
            dset_doc = group_documents.create_dataset("{}".format(ind), data=doc.words)
            dset_doc.attrs['text'] = doc.body
            dset_doc.attrs['authorID'] = doc.authorID
            dset_doc.attrs['author_index'] = doc.author_index
            dset_doc.attrs['textID'] = doc.ID
            dset_doc.attrs['number of words'] = doc.N
                      
            # Copy word number.
            N.append(doc.N)
            # Copy author index.
            authors_vec.append(doc.author_index)

            
        dset_authors = corpus_group.create_dataset('authors', data=authors_vec)   # Vector of author indices.
        dset_authors.attrs['info'] = 'Vector of author indices containing an author integer index for every document.'
        dset_N = corpus_group.create_dataset('N', data=N)                     # Vector of word counts.
        dset_N.attrs['info'] = 'Vector of word counts for every document.'
        dset_stopword_indicator = corpus_group.create_dataset('stopword_indicator', data=self.is_stopword)
        dset_stopword_indicator.attrs['info'] = 'Vector of stopword indicators for every word in the vocabulary. 0 if word at index is not a stop word. 1 if it is.'
        
        
        dset_V = corpus_group.create_dataset('V', data=self.V)
        dset_V.attrs['info'] = 'Number of words in the vocabulary.'
        dset_D = corpus_group.create_dataset('D', data=self.D)
        dset_D.attrs['info'] = 'Number of documents.'
        dset_A = corpus_group.create_dataset('A', data=self.A)
        dset_A.attrs['info'] = 'Number of authors.'
        
        vocab_string = ""
        for word in self.vocab:
            vocab_string = vocab_string + word + " "
#         dset_vocab = corpus_group.create_dataset('Vocabulary', data=self.vocab)
#         dset_vocab.attrs['info'] = 'Vocabulary vector. Words in form of index refer to this vocabulary vector.'
        dset_V.attrs['vocabulary string'] = vocab_string
        
        # Store number of documents per author.
        self.text_count = np.zeros(self.A, dtype=np.int32)
        for doc in self.documents:
            self.text_count[doc.author_index] += 1
        
        dset_text_count = corpus_group.create_dataset('text_count', data=self.text_count)
        dset_text_count.attrs['info'] = 'Number documents of each author.'
        
        group_authors = corpus_group.create_group('author_ids')
        group_authors.attrs['info'] = 'Group of authors, each author has a dataset with the document count and with the author id from the original dataset as attribute.'
        
        for key in self.authors:
            author_ind = self.authors[key]
            dset_author = group_authors.create_dataset("{}".format(author_ind), data=self.text_count[author_ind]) 
            dset_author.attrs['authorID'] = key
        
    def print_all_author_texts(self, a):
        '''
        Print all texts of a given author.
        '''
        
        # Find all texts of author a.
        texts = [];
        for doc in self.documents:
            if (doc.author_index == a):
                texts.append(doc)
        
        for doc in texts:
            print("---------------------------------------------------------------------")
            print(doc.body)
        
class Document:
    '''
    Class to store and process document data
    '''
    
    # Set with punctuation characters.
    punct = set([',','.','@',"'"])
    punct_to_del = set(string.punctuation)
    for p in punct:
        punct_to_del.remove(p)
    
    for n in ['0','1','2','3','4','5','6','7','8','9']:
        punct_to_del.add(n)
    

    
    def __init__(self, ID):
        self.body = ""
        self.ID = ID
        self.index = 0  # Integer index for use in the model.
        self.authorID = ""
        self.author_index = -1    # Variable for the author index according to the set of corpus authors.
        # Initialize list for words in form of a string.
        self.tokens = []
        # Initialize list for words in form of an index.
        self.words = []
        # Number of words in the document.
        self.N = 0
        
        
    def word_tokenize(self):
        '''
        Tokenize words using the word tokenizer from nltk.
        '''
        
        # Remove "<NAME/># from the text.
        clean_body = self.body.replace("<NAME/>","")
        
        # Tokenize words.
        self.tokens = nltk.word_tokenize(clean_body)
        
        # Update number of words.
        self.N = len(self.tokens)
        
        # Normalize words.
        for i in range(self.N):
            self.tokens[i] = self.tokens[i].lower()
        
        
        
    def word_tokenize_own(self):
        ''' 
        Construct a list with words using the text in self.body.
        Words are not necessarily in the original sequence.
        Punctuation at the beginning and the end gets removed.
        '''
        
        clean_body = self.body
        # Remove all unwanted punctuation.
        for p in self.punct_to_del:
            clean_body = clean_body.replace(p, "")
        
        words_splitted = clean_body.split()
        
        # Loop over words.
        for word in words_splitted:
            # Ignore names specified by <NAME/>, for PAN11.
            word = word.replace("<NAME/>","")
     
            
            # Make all characters lower case.
            word = word.lower()
            
            # Set flags to false.
            is_left_punctuation_removed = False
            is_right_punctuation_removed = False
            
            # Loop until flags is True.
            while not(is_left_punctuation_removed and is_right_punctuation_removed):
                # Check beginning for punctuation character.
                if len(word) > 0 and word[0] in self.punct:
                    self.tokens.append(word[0])
                    if len(word) > 0:
                        word = word[1:]
                else:
                    is_left_punctuation_removed = True
                # Check end for punctuation character.
                if len(word) > 0 and word[-1] in self.punct:
                    self.tokens.append(word[-1])
                    if len(word) > 0:
                        word = word[:-1]
                else:
                    is_right_punctuation_removed = True
                    
            if len(word) > 0:
                self.tokens.append(word)
        
        # Update number of words.
        self.N = len(self.tokens)
                        